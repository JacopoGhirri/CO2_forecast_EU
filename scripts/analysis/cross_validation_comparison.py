"""
Cross-validation comparison of model variants (Table 3).

Performs repeated random train/val/test splitting to compare six emission
prediction approaches on identical data partitions:

  1. **Baseline (No latent space)** — Direct prediction from raw concatenated
     inputs (x_t, x_{t-1}, c_t, c_{t-1}) to emission deltas, bypassing any
     dimensionality reduction.

  2. **PCA** — Deterministic linear reduction of input features via Principal
     Component Analysis, followed by latent forecasting and emission prediction.

  3. **KPCA** — Kernel PCA (RBF kernel) as a nonlinear deterministic reduction.

  4. **ICA** — Independent Component Analysis as a deterministic reduction.

  5. **VAE (no context)** — The full VAE-based pipeline but *without* any
     context variables (GDP, population, climate, HDD/CDD).  This isolates the
     contribution of the probabilistic latent space alone.

  6. **VAE + Context Variables (Final)** — The complete pipeline as described
     in the paper, with VAE encoding and all context variables included.

For each split every model is trained from scratch (including fitting PCA/KPCA/ICA
on the training fold only) so that no information leaks across folds.

Metrics reported per output sector and aggregated:
    - MAE ↓  (Mean Absolute Error)
    - MSE ↓  (Mean Squared Error)
    - Pearson ρ ↑  (correlation between predicted and true emission deltas)
    - ℒ_pred ↓  (uncertainty-aware prediction loss, for variants that support it)

Usage:
    python -m scripts.analysis.cross_validation_comparison

Outputs:
    - outputs/tables/table3_cv_comparison.csv   — per-split, per-metric summary
    - stdout: formatted table matching the paper's Table 3 layout

Prerequisites:
    - Processed dataset: data/pytorch_datasets/unified_dataset.pkl
      (generated by scripts/training/train_vae.py on first run)
    - sklearn (for PCA, KernelPCA, FastICA)

Reference:
    Table 3 in the paper compares model variants across 100 random
    train/val/test (70/15/15) splits.
"""

import multiprocessing as mp
import random
from collections import deque
from pathlib import Path
from typing import Literal

import numpy as np
import pandas as pd
import torch
import torch.nn as nn
from scipy.stats import pearsonr
from sklearn.decomposition import PCA, FastICA, KernelPCA
from torch.utils.data import DataLoader, Subset

from config.data.output_configs import output_configs
from scripts.elements.datasets import DatasetPrediction
from scripts.elements.models import (
    Decoder,
    EmissionPredictor,
    Encoder,
    FullPredictionModel,
    LatentForecaster,
    VAEModel,
    reparameterize,
    uncertainty_aware_mse_loss,
    vae_loss,
)
from scripts.utils import (
    check_nan_gradients,
    init_weights,
    load_config,
    load_dataset,
    save_dataset,
)

# =============================================================================
# Configuration
# =============================================================================

# Reproducibility
SEED = 0

# Cross-validation settings
N_SPLITS = 100
TRAIN_RATIO = 0.70
VAL_RATIO = 0.15
TEST_RATIO = 0.15
BATCH_SIZE = 128

# Training epochs per stage.  The VAE and forecaster are trained jointly
# within each split, so we keep epoch counts moderate for tractability.
EPOCHS_VAE = 500
EPOCHS_PREDICTOR = 500
EPOCHS_FORECASTER = 500
EPOCHS_DIRECT = 500  # For the Baseline (direct) variant

# Latent dimension used by all latent-space variants.
# Should match the VAE config for the "Final" model.
LATENT_DIM = 10

# Deterministic reduction target dimension (same as LATENT_DIM for fair comparison)
REDUCTION_DIM = LATENT_DIM

# Device
DEVICE = "cuda" if torch.cuda.is_available() else "cpu"

# Paths
DATASET_PATH = Path("data/pytorch_datasets/unified_dataset.pkl")
VARIABLE_FILE = Path("config/data/variable_selection.txt")
OUTPUT_DIR = Path("outputs/tables")

# EU27 country codes
EU27_COUNTRIES = [
    "AT",
    "BE",
    "BG",
    "HR",
    "CY",
    "CZ",
    "DK",
    "EE",
    "EL",
    "FI",
    "FR",
    "DE",
    "HU",
    "IE",
    "IT",
    "LV",
    "LT",
    "LU",
    "MT",
    "NL",
    "PL",
    "PT",
    "RO",
    "SK",
    "SI",
    "ES",
    "SE",
]

# Emission sectors (from output_configs)
SECTORS = ["HeatingCooling", "Industry", "Land", "Mobility", "Other", "Power"]

# Model variant identifier type
VariantName = Literal["baseline", "pca", "kpca", "ica", "vae_no_context", "vae_final"]


# =============================================================================
# Helper: smoothed early-stopping tracker
# =============================================================================


class EarlyStopper:
    """
    Tracks smoothed validation loss and stores best model weights.

    Uses a rolling window to smooth noisy validation curves and saves
    a copy of the model state dict whenever the smoothed loss improves.
    """

    def __init__(self, window: int = 10):
        self.history: deque[float] = deque(maxlen=window)
        self.best_smooth: float = float("inf")
        self.best_weights: dict | None = None

    def step(self, val_loss: float, model: nn.Module) -> None:
        """Record a validation loss and update best weights if improved."""
        self.history.append(val_loss)
        smooth = sum(self.history) / len(self.history)
        if smooth < self.best_smooth:
            self.best_smooth = smooth
            self.best_weights = {
                k: v.cpu().clone() for k, v in model.state_dict().items()
            }

    def restore(self, model: nn.Module) -> None:
        """Load the best weights back into the model."""
        if self.best_weights is not None:
            model.load_state_dict(self.best_weights)
            model.to(DEVICE)


# =============================================================================
# Dataset loading
# =============================================================================


def get_or_create_dataset() -> DatasetPrediction:
    """
    Loads the cached prediction dataset, or creates it from raw CSVs.

    If the cached dataset is a DatasetUnified (not DatasetPrediction),
    it is converted by changing its class, since DatasetPrediction only
    overrides __getitem__.
    """
    if DATASET_PATH.exists():
        print(f"Loading cached dataset from {DATASET_PATH}")
        ds = load_dataset(DATASET_PATH)
        # The cached file may be a DatasetUnified saved by train_vae.py.
        # DatasetPrediction only overrides __getitem__, so we can safely
        # change the class to get the 6-tuple behavior.
        if not isinstance(ds, DatasetPrediction):
            ds.__class__ = DatasetPrediction
        return ds

    with open(VARIABLE_FILE) as f:
        nested_variables = [line.strip() for line in f if line.strip()]

    print("Creating dataset from raw CSVs...")
    dataset = DatasetPrediction(
        path_csvs="data/full_timeseries/",
        output_configs=output_configs,
        select_years=np.arange(2010, 2023 + 1),
        select_geo=EU27_COUNTRIES,
        nested_variables=nested_variables,
        with_cuda=False,
        scaling_type="normalization",
    )
    save_dataset(dataset, DATASET_PATH)
    print(f"Dataset saved to {DATASET_PATH}")
    return dataset


# =============================================================================
# Variant-specific builders
# =============================================================================


def _build_vae(input_dim: int) -> VAEModel:
    """Builds a fresh VAE with the standard architecture from the paper."""
    # We use the same architecture as the main paper config but allow
    # the caller to vary input_dim (needed when context is excluded).
    config = load_config("config/models/vae_config.yaml")
    encoder = Encoder(
        input_dim=input_dim,
        latent_dim=config.vae_latent_dim,
        num_blocks=config.vae_num_blocks,
        dim_blocks=config.vae_dim_blocks,
        activation=config.vae_activation,
        normalization=config.vae_normalization,
        dropout=config.vae_dropouts,
        input_dropout=config.vae_input_dropouts,
    )
    decoder = Decoder(
        input_dim=input_dim,
        latent_dim=config.vae_latent_dim,
        num_blocks=config.vae_num_blocks,
        dim_blocks=config.vae_dim_blocks,
        activation=config.vae_activation,
        normalization=config.vae_normalization,
        dropout=config.vae_dropouts,
    )
    vae = VAEModel(encoder, decoder)
    vae.apply(init_weights)
    return vae


def _build_predictor(input_dim: int, uncertainty: bool = True) -> EmissionPredictor:
    """Builds a fresh emission predictor."""
    config = load_config("config/models/co2_predictor_config.yaml")
    predictor = EmissionPredictor(
        input_dim=input_dim,
        output_configs=output_configs,
        num_blocks=config.pred_num_blocks,
        dim_block=config.pred_dim_block,
        width_block=config.pred_width_block,
        activation=config.pred_activation,
        normalization=config.pred_normalization,
        dropout=config.pred_dropouts,
        uncertainty=uncertainty,
    )
    predictor.apply(init_weights)
    return predictor


def _build_forecaster(input_dim: int, latent_dim: int) -> LatentForecaster:
    """Builds a fresh latent forecaster."""
    config = load_config("config/models/latent_forecaster_config.yaml")
    forecaster = LatentForecaster(
        input_dim=input_dim,
        latent_dim=latent_dim,
        num_blocks=config.forecast_num_blocks,
        dim_block=config.forecast_dim_block,
        width_block=config.forecast_width_block,
        activation=config.forecast_activation,
        normalization=config.forecast_normalization,
        dropout=config.forecast_dropouts,
    )
    forecaster.apply(init_weights)
    return forecaster


# =============================================================================
# Training loops
# =============================================================================


def _train_vae(
    vae: VAEModel,
    train_loader: DataLoader,
    val_loader: DataLoader,
    epochs: int,
    extract_x: callable,
) -> VAEModel:
    """
    Trains the VAE on reconstruction + KL loss.

    Args:
        vae: Fresh VAEModel on DEVICE.
        train_loader / val_loader: DataLoaders yielding dataset tuples.
        epochs: Number of training epochs.
        extract_x: Callable that takes a batch tuple and returns the
            input tensor x_t to reconstruct.  This allows the same loop
            to work for both full-input and reduced-input variants.

    Returns:
        VAE with best weights restored.
    """
    config = load_config("config/models/vae_config.yaml")
    weight_recon = config.vae_wr
    weight_kl = config.vae_wd

    optimizer = torch.optim.AdamW(
        vae.parameters(),
        lr=config.vae_lr,
        weight_decay=config.vae_weight_decay,
        eps=1e-6,
    )
    stopper = EarlyStopper(window=25)

    for _epoch in range(epochs):
        # --- Train ---
        vae.train()
        for batch in train_loader:
            x = extract_x(batch).to(DEVICE)
            optimizer.zero_grad()
            x_hat, mean, log_var = vae(x)
            recon, kl = vae_loss(x, x_hat, mean, log_var)
            loss = weight_recon * recon + weight_kl * kl
            loss.backward()
            torch.nn.utils.clip_grad_norm_(vae.parameters(), 1.0)
            check_nan_gradients(vae)
            optimizer.step()

        # --- Validate ---
        vae.eval()
        val_loss = 0.0
        with torch.inference_mode():
            for batch in val_loader:
                x = extract_x(batch).to(DEVICE)
                x_hat, mean, log_var = vae(x)
                recon, kl = vae_loss(x, x_hat, mean, log_var)
                val_loss += (weight_recon * recon + weight_kl * kl).item()
        val_loss /= len(val_loader)
        stopper.step(val_loss, vae)

    stopper.restore(vae)
    return vae


def _train_predictor_with_vae(
    full_model: FullPredictionModel,
    train_loader: DataLoader,
    val_loader: DataLoader,
    epochs: int,
    loss_mode: str = "factor",
) -> FullPredictionModel:
    """
    Trains the emission predictor while keeping VAE weights frozen.

    The full_model wraps a pre-trained VAE + a fresh predictor.
    Only the predictor parameters are updated.

    Args:
        full_model: FullPredictionModel on DEVICE.
        train_loader / val_loader: DataLoaders (DatasetPrediction tuples).
        epochs: Training epochs.
        loss_mode: Mode for uncertainty_aware_mse_loss.

    Returns:
        Model with best predictor weights restored.
    """
    # Freeze VAE
    for p in full_model.vae.parameters():
        p.requires_grad = False

    pred_config = load_config("config/models/co2_predictor_config.yaml")
    optimizer = torch.optim.AdamW(
        full_model.predictor.parameters(),
        lr=pred_config.pred_lr,
        weight_decay=pred_config.pred_wd,
        eps=1e-6,
    )
    stopper = EarlyStopper(window=25)

    for _epoch in range(epochs):
        full_model.train()
        for batch in train_loader:
            x_t, c_t, y_t, x_t1, c_t1, y_t1 = [b.to(DEVICE) for b in batch]
            optimizer.zero_grad()
            delta_pred, unc, *_ = full_model(x_t, x_t1, c_t, c_t1)
            delta_true = y_t - y_t1
            loss = uncertainty_aware_mse_loss(
                delta_true, delta_pred, unc, mode=loss_mode
            )
            loss.backward()
            torch.nn.utils.clip_grad_norm_(full_model.parameters(), 1.0)
            check_nan_gradients(full_model)
            optimizer.step()

        full_model.eval()
        val_loss = 0.0
        with torch.inference_mode():
            for batch in val_loader:
                x_t, c_t, y_t, x_t1, c_t1, y_t1 = [b.to(DEVICE) for b in batch]
                delta_pred, unc, *_ = full_model(x_t, x_t1, c_t, c_t1)
                delta_true = y_t - y_t1
                val_loss += uncertainty_aware_mse_loss(
                    delta_true, delta_pred, unc, mode=loss_mode
                ).item()
        val_loss /= len(val_loader)
        stopper.step(val_loss, full_model)

    stopper.restore(full_model)
    return full_model


def _train_direct_predictor(
    predictor: EmissionPredictor,
    train_loader: DataLoader,
    val_loader: DataLoader,
    epochs: int,
    context_dim: int,
    loss_mode: str = "factor",
) -> EmissionPredictor:
    """
    Trains the Baseline (direct) predictor that bypasses any latent space.

    Input to the predictor is the raw concatenation:
        [x_t, c_t, x_{t-1}, c_{t-1}]

    Args:
        predictor: Fresh EmissionPredictor on DEVICE (uncertainty=True).
        train_loader / val_loader: DataLoaders (DatasetPrediction tuples).
        epochs: Training epochs.
        context_dim: Dimensionality of context vectors (for concatenation).
        loss_mode: Mode for uncertainty_aware_mse_loss.

    Returns:
        Predictor with best weights restored.
    """
    pred_config = load_config("config/models/co2_predictor_config.yaml")
    optimizer = torch.optim.AdamW(
        predictor.parameters(),
        lr=pred_config.pred_lr,
        weight_decay=pred_config.pred_wd,
        eps=1e-6,
    )
    stopper = EarlyStopper(window=25)

    for _epoch in range(epochs):
        predictor.train()
        for batch in train_loader:
            x_t, c_t, y_t, x_t1, c_t1, y_t1 = [b.to(DEVICE) for b in batch]
            # Concatenate raw inputs and context from both time steps
            inp = torch.cat([x_t, c_t, x_t1, c_t1], dim=1)
            optimizer.zero_grad()
            delta_pred, unc = predictor(inp)
            delta_true = y_t - y_t1
            loss = uncertainty_aware_mse_loss(
                delta_true, delta_pred, unc, mode=loss_mode
            )
            loss.backward()
            torch.nn.utils.clip_grad_norm_(predictor.parameters(), 1.0)
            check_nan_gradients(predictor)
            optimizer.step()

        predictor.eval()
        val_loss = 0.0
        with torch.inference_mode():
            for batch in val_loader:
                x_t, c_t, y_t, x_t1, c_t1, y_t1 = [b.to(DEVICE) for b in batch]
                inp = torch.cat([x_t, c_t, x_t1, c_t1], dim=1)
                delta_pred, unc = predictor(inp)
                delta_true = y_t - y_t1
                val_loss += uncertainty_aware_mse_loss(
                    delta_true, delta_pred, unc, mode=loss_mode
                ).item()
        val_loss /= len(val_loader)
        stopper.step(val_loss, predictor)

    stopper.restore(predictor)
    return predictor


# =============================================================================
# Deterministic latent space helpers
# =============================================================================


def _fit_deterministic_reduction(
    method: Literal["pca", "kpca", "ica"],
    X_train: np.ndarray,
    n_components: int,
) -> object:
    """
    Fits a deterministic dimensionality reduction on training data.

    Args:
        method: One of "pca", "kpca", "ica".
        X_train: Training data array, shape (N_train, D).
        n_components: Target latent dimensionality.

    Returns:
        Fitted sklearn transformer with .transform() method.
    """
    if method == "pca":
        reducer = PCA(n_components=n_components, random_state=SEED)
    elif method == "kpca":
        reducer = KernelPCA(
            n_components=n_components,
            kernel="rbf",
            random_state=SEED,
            fit_inverse_transform=False,
        )
    elif method == "ica":
        reducer = FastICA(
            n_components=n_components,
            random_state=SEED,
            max_iter=500,
        )
    else:
        raise ValueError(f"Unknown method: {method}")

    reducer.fit(X_train)
    return reducer


def _train_predictor_with_deterministic_latent(
    reducer,
    predictor: EmissionPredictor,
    train_loader: DataLoader,
    val_loader: DataLoader,
    dataset: DatasetPrediction,
    epochs: int,
    loss_mode: str = "factor",
) -> EmissionPredictor:
    """
    Trains emission predictor using a pre-fitted deterministic reduction.

    The reducer replaces the VAE encoder: for each sample we transform
    x_t and x_{t-1} through the reducer to get "latent" vectors z_t, z_{t-1},
    then concatenate with context as [z_t, c_t, z_{t-1}, c_{t-1}].

    Args:
        reducer: Fitted sklearn transformer with .transform().
        predictor: Fresh EmissionPredictor on DEVICE.
        train_loader / val_loader: DataLoaders.
        dataset: The full dataset (needed to get context_dim).
        epochs: Training epochs.
        loss_mode: Mode for uncertainty_aware_mse_loss.

    Returns:
        Predictor with best weights restored.
    """
    pred_config = load_config("config/models/co2_predictor_config.yaml")
    optimizer = torch.optim.AdamW(
        predictor.parameters(),
        lr=pred_config.pred_lr,
        weight_decay=pred_config.pred_wd,
        eps=1e-6,
    )
    stopper = EarlyStopper(window=25)

    def _encode_batch(x: torch.Tensor) -> torch.Tensor:
        """Apply sklearn reducer to a batch (CPU numpy round-trip)."""
        z = reducer.transform(x.cpu().numpy())
        return torch.tensor(z, dtype=torch.float32, device=DEVICE)

    for _epoch in range(epochs):
        predictor.train()
        for batch in train_loader:
            x_t, c_t, y_t, x_t1, c_t1, y_t1 = [b.to(DEVICE) for b in batch]
            z_t = _encode_batch(x_t)
            z_t1 = _encode_batch(x_t1)
            inp = torch.cat([z_t, c_t, z_t1, c_t1], dim=1)

            optimizer.zero_grad()
            delta_pred, unc = predictor(inp)
            delta_true = y_t - y_t1
            loss = uncertainty_aware_mse_loss(
                delta_true, delta_pred, unc, mode=loss_mode
            )
            loss.backward()
            torch.nn.utils.clip_grad_norm_(predictor.parameters(), 1.0)
            check_nan_gradients(predictor)
            optimizer.step()

        predictor.eval()
        val_loss = 0.0
        with torch.inference_mode():
            for batch in val_loader:
                x_t, c_t, y_t, x_t1, c_t1, y_t1 = [b.to(DEVICE) for b in batch]
                z_t = _encode_batch(x_t)
                z_t1 = _encode_batch(x_t1)
                inp = torch.cat([z_t, c_t, z_t1, c_t1], dim=1)
                delta_pred, unc = predictor(inp)
                delta_true = y_t - y_t1
                val_loss += uncertainty_aware_mse_loss(
                    delta_true, delta_pred, unc, mode=loss_mode
                ).item()
        val_loss /= len(val_loader)
        stopper.step(val_loss, predictor)

    stopper.restore(predictor)
    return predictor


# =============================================================================
# Evaluation
# =============================================================================


def _evaluate_vae_variant(
    full_model: FullPredictionModel,
    test_loader: DataLoader,
    loss_mode: str = "factor",
) -> dict:
    """
    Evaluates a VAE-based variant (VAE no-context or VAE+Context) on the
    test set.

    Returns per-sector MAE, MSE, Pearson ρ, ℒ_pred, and their averages.
    """
    full_model.eval()
    all_preds, all_targets, all_uncs = [], [], []

    with torch.inference_mode():
        for batch in test_loader:
            x_t, c_t, y_t, x_t1, c_t1, y_t1 = [b.to(DEVICE) for b in batch]
            delta_pred, unc, *_ = full_model(x_t, x_t1, c_t, c_t1)
            delta_true = y_t - y_t1
            all_preds.append(delta_pred.cpu())
            all_targets.append(delta_true.cpu())
            all_uncs.append(unc.cpu())

    preds = torch.cat(all_preds).numpy()
    targets = torch.cat(all_targets).numpy()
    uncs = torch.cat(all_uncs).numpy()

    return _compute_metrics(preds, targets, uncs, loss_mode)


def _evaluate_direct_variant(
    predictor: EmissionPredictor,
    test_loader: DataLoader,
    loss_mode: str = "factor",
) -> dict:
    """
    Evaluates the Baseline (direct) variant on the test set.
    """
    predictor.eval()
    all_preds, all_targets, all_uncs = [], [], []

    with torch.inference_mode():
        for batch in test_loader:
            x_t, c_t, y_t, x_t1, c_t1, y_t1 = [b.to(DEVICE) for b in batch]
            inp = torch.cat([x_t, c_t, x_t1, c_t1], dim=1)
            delta_pred, unc = predictor(inp)
            delta_true = y_t - y_t1
            all_preds.append(delta_pred.cpu())
            all_targets.append(delta_true.cpu())
            all_uncs.append(unc.cpu())

    preds = torch.cat(all_preds).numpy()
    targets = torch.cat(all_targets).numpy()
    uncs = torch.cat(all_uncs).numpy()

    return _compute_metrics(preds, targets, uncs, loss_mode)


def _evaluate_deterministic_variant(
    reducer,
    predictor: EmissionPredictor,
    test_loader: DataLoader,
    loss_mode: str = "factor",
) -> dict:
    """
    Evaluates a deterministic reduction variant (PCA/KPCA/ICA) on the test set.
    """
    predictor.eval()
    all_preds, all_targets, all_uncs = [], [], []

    def _encode_batch(x: torch.Tensor) -> torch.Tensor:
        z = reducer.transform(x.cpu().numpy())
        return torch.tensor(z, dtype=torch.float32, device=DEVICE)

    with torch.inference_mode():
        for batch in test_loader:
            x_t, c_t, y_t, x_t1, c_t1, y_t1 = [b.to(DEVICE) for b in batch]
            z_t = _encode_batch(x_t)
            z_t1 = _encode_batch(x_t1)
            inp = torch.cat([z_t, c_t, z_t1, c_t1], dim=1)
            delta_pred, unc = predictor(inp)
            delta_true = y_t - y_t1
            all_preds.append(delta_pred.cpu())
            all_targets.append(delta_true.cpu())
            all_uncs.append(unc.cpu())

    preds = torch.cat(all_preds).numpy()
    targets = torch.cat(all_targets).numpy()
    uncs = torch.cat(all_uncs).numpy()

    return _compute_metrics(preds, targets, uncs, loss_mode)


def _compute_metrics(
    preds: np.ndarray,
    targets: np.ndarray,
    uncs: np.ndarray,
    loss_mode: str,
) -> dict:
    """
    Computes MAE, MSE, Pearson ρ, and ℒ_pred for each output sector and
    their averages.

    Args:
        preds: Predicted emission deltas, shape (N, n_sectors).
        targets: True emission deltas, shape (N, n_sectors).
        uncs: Log-variance uncertainty estimates, shape (N, n_sectors).
        loss_mode: Mode for the uncertainty-aware loss computation.

    Returns:
        Dictionary with keys like "mae", "mse", "corr", "loss_pred",
        plus per-sector variants "mae_0", "mae_1", etc.
    """
    n_sectors = preds.shape[1]
    metrics = {}

    maes, mses, corrs = [], [], []
    for i in range(n_sectors):
        mae_i = float(np.mean(np.abs(preds[:, i] - targets[:, i])))
        mse_i = float(np.mean((preds[:, i] - targets[:, i]) ** 2))
        try:
            corr_i, _ = pearsonr(targets[:, i], preds[:, i])
            corr_i = float(corr_i)
        except ValueError:
            corr_i = float("nan")

        metrics[f"mae_{i}"] = mae_i
        metrics[f"mse_{i}"] = mse_i
        metrics[f"corr_{i}"] = corr_i
        maes.append(mae_i)
        mses.append(mse_i)
        corrs.append(corr_i)

    metrics["mae"] = float(np.mean(maes))
    metrics["mse"] = float(np.mean(mses))
    metrics["corr"] = float(np.nanmean(corrs))

    # ℒ_pred: uncertainty-aware loss on the full test set
    preds_t = torch.tensor(preds, dtype=torch.float32)
    targets_t = torch.tensor(targets, dtype=torch.float32)
    uncs_t = torch.tensor(uncs, dtype=torch.float32)
    loss_pred = uncertainty_aware_mse_loss(targets_t, preds_t, uncs_t, mode=loss_mode)
    metrics["loss_pred"] = float(loss_pred.item())

    return metrics


# =============================================================================
# Per-split runner for each variant
# =============================================================================


def run_baseline(
    dataset: DatasetPrediction,
    train_idx: list[int],
    val_idx: list[int],
    test_idx: list[int],
) -> dict:
    """
    Baseline (no latent space): predict emission deltas directly from
    the concatenation [x_t, c_t, x_{t-1}, c_{t-1}].
    """
    input_dim = dataset.input_df.shape[1]
    context_dim = dataset.context_df.shape[1]

    # Predictor input: 2 * (input_dim + context_dim)
    predictor = _build_predictor(
        input_dim=2 * (input_dim + context_dim), uncertainty=True
    ).to(DEVICE)

    train_loader = DataLoader(
        Subset(dataset, train_idx), batch_size=BATCH_SIZE, shuffle=True
    )
    val_loader = DataLoader(
        Subset(dataset, val_idx), batch_size=BATCH_SIZE, shuffle=False
    )
    test_loader = DataLoader(
        Subset(dataset, test_idx), batch_size=BATCH_SIZE, shuffle=False
    )

    predictor = _train_direct_predictor(
        predictor, train_loader, val_loader, EPOCHS_DIRECT, context_dim
    )
    return _evaluate_direct_variant(predictor, test_loader)


def run_deterministic_latent(
    method: Literal["pca", "kpca", "ica"],
    dataset: DatasetPrediction,
    train_idx: list[int],
    val_idx: list[int],
    test_idx: list[int],
) -> dict:
    """
    Deterministic latent space variant (PCA / KPCA / ICA).

    Fits the reduction on training inputs only, then trains a predictor
    that receives [z_t, c_t, z_{t-1}, c_{t-1}].
    """
    dataset.input_df.shape[1]
    context_dim = dataset.context_df.shape[1]

    # Fit reduction on training inputs
    X_train = dataset.input_df[train_idx].cpu().numpy()
    reducer = _fit_deterministic_reduction(method, X_train, REDUCTION_DIM)

    # Predictor input: 2 * (REDUCTION_DIM + context_dim)
    predictor = _build_predictor(
        input_dim=2 * (REDUCTION_DIM + context_dim), uncertainty=True
    ).to(DEVICE)

    train_loader = DataLoader(
        Subset(dataset, train_idx), batch_size=BATCH_SIZE, shuffle=True
    )
    val_loader = DataLoader(
        Subset(dataset, val_idx), batch_size=BATCH_SIZE, shuffle=False
    )
    test_loader = DataLoader(
        Subset(dataset, test_idx), batch_size=BATCH_SIZE, shuffle=False
    )

    predictor = _train_predictor_with_deterministic_latent(
        reducer, predictor, train_loader, val_loader, dataset, EPOCHS_PREDICTOR
    )
    return _evaluate_deterministic_variant(reducer, predictor, test_loader)


def run_vae_variant(
    dataset: DatasetPrediction,
    train_idx: list[int],
    val_idx: list[int],
    test_idx: list[int],
    include_context: bool,
) -> dict:
    """
    VAE-based variant.

    When include_context=False, context vectors are zeroed out so that the
    predictor only sees [z_t, 0, z_{t-1}, 0].  This tests the contribution
    of the probabilistic latent space alone.

    When include_context=True, this is the full "Final" model.

    The VAE is trained on the training fold, then the predictor is trained
    with frozen VAE weights.
    """
    input_dim = dataset.input_df.shape[1]
    context_dim = dataset.context_df.shape[1]

    # Effective context dim: 0 if excluding context, else full
    effective_context_dim = context_dim if include_context else 0

    # --- Step 1: Train VAE on training fold ---
    vae = _build_vae(input_dim).to(DEVICE)

    train_loader = DataLoader(
        Subset(dataset, train_idx), batch_size=BATCH_SIZE, shuffle=True
    )
    val_loader = DataLoader(
        Subset(dataset, val_idx), batch_size=BATCH_SIZE, shuffle=False
    )
    test_loader = DataLoader(
        Subset(dataset, test_idx), batch_size=BATCH_SIZE, shuffle=False
    )

    # VAE trains on x_t only (index 0 of the tuple)
    vae = _train_vae(
        vae,
        train_loader,
        val_loader,
        EPOCHS_VAE,
        extract_x=lambda batch: batch[0],  # x_t
    )

    # --- Step 2: Build and train predictor with frozen VAE ---
    vae_config = load_config("config/models/vae_config.yaml")
    latent_dim = vae_config.vae_latent_dim

    # Predictor input: 2 * (latent_dim + effective_context_dim)
    predictor = _build_predictor(
        input_dim=2 * (latent_dim + effective_context_dim), uncertainty=True
    ).to(DEVICE)

    full_model = FullPredictionModel(vae=vae, predictor=predictor).to(DEVICE)

    if not include_context:
        # We need a custom training loop that zeros out context.
        # We wrap the FullPredictionModel forward to mask context.
        full_model = _train_predictor_no_context(
            full_model, train_loader, val_loader, EPOCHS_PREDICTOR
        )
        return _evaluate_vae_no_context(full_model, test_loader)
    else:
        full_model = _train_predictor_with_vae(
            full_model, train_loader, val_loader, EPOCHS_PREDICTOR
        )
        return _evaluate_vae_variant(full_model, test_loader)


# =============================================================================
# Special handling for "VAE no context" variant
# =============================================================================
# When context is excluded, we cannot use FullPredictionModel.forward()
# directly because it expects context tensors.  Instead we manually call
# the encoder and predictor, passing zero-width or zero-valued context.


def _train_predictor_no_context(
    full_model: FullPredictionModel,
    train_loader: DataLoader,
    val_loader: DataLoader,
    epochs: int,
) -> FullPredictionModel:
    """
    Trains predictor without context variables.

    The predictor receives [z_t, z_{t-1}] (no context concatenated).
    The VAE encoder is frozen.
    """
    for p in full_model.vae.parameters():
        p.requires_grad = False

    pred_config = load_config("config/models/co2_predictor_config.yaml")
    optimizer = torch.optim.AdamW(
        full_model.predictor.parameters(),
        lr=pred_config.pred_lr,
        weight_decay=pred_config.pred_wd,
        eps=1e-6,
    )
    stopper = EarlyStopper(window=25)

    def _forward_no_ctx(batch):
        """Encode x_t and x_{t-1}, concatenate latents (no context)."""
        x_t, _c_t, y_t, x_t1, _c_t1, y_t1 = [b.to(DEVICE) for b in batch]
        mean_t, logvar_t = full_model.encoder(x_t)
        mean_t1, logvar_t1 = full_model.encoder(x_t1)
        z_t = reparameterize(mean_t, torch.exp(0.5 * logvar_t))
        z_t1 = reparameterize(mean_t1, torch.exp(0.5 * logvar_t1))
        # Predictor input: [z_t, z_{t-1}]  (no context)
        inp = torch.cat([z_t, z_t1], dim=1)
        delta_pred, unc = full_model.predictor(inp)
        delta_true = y_t - y_t1
        return delta_pred, unc, delta_true

    for _epoch in range(epochs):
        full_model.train()
        # Keep encoder in eval to disable dropout in encoder
        full_model.vae.eval()
        for batch in train_loader:
            optimizer.zero_grad()
            delta_pred, unc, delta_true = _forward_no_ctx(batch)
            loss = uncertainty_aware_mse_loss(
                delta_true, delta_pred, unc, mode="factor"
            )
            loss.backward()
            torch.nn.utils.clip_grad_norm_(full_model.parameters(), 1.0)
            check_nan_gradients(full_model)
            optimizer.step()

        full_model.eval()
        val_loss = 0.0
        with torch.inference_mode():
            for batch in val_loader:
                delta_pred, unc, delta_true = _forward_no_ctx(batch)
                val_loss += uncertainty_aware_mse_loss(
                    delta_true, delta_pred, unc, mode="factor"
                ).item()
        val_loss /= len(val_loader)
        stopper.step(val_loss, full_model)

    stopper.restore(full_model)
    return full_model


def _evaluate_vae_no_context(
    full_model: FullPredictionModel,
    test_loader: DataLoader,
) -> dict:
    """Evaluates the VAE no-context variant."""
    full_model.eval()
    all_preds, all_targets, all_uncs = [], [], []

    with torch.inference_mode():
        for batch in test_loader:
            x_t, _c_t, y_t, x_t1, _c_t1, y_t1 = [b.to(DEVICE) for b in batch]
            mean_t, logvar_t = full_model.encoder(x_t)
            mean_t1, logvar_t1 = full_model.encoder(x_t1)
            z_t = reparameterize(mean_t, torch.exp(0.5 * logvar_t))
            z_t1 = reparameterize(mean_t1, torch.exp(0.5 * logvar_t1))
            inp = torch.cat([z_t, z_t1], dim=1)
            delta_pred, unc = full_model.predictor(inp)
            delta_true = y_t - y_t1
            all_preds.append(delta_pred.cpu())
            all_targets.append(delta_true.cpu())
            all_uncs.append(unc.cpu())

    preds = torch.cat(all_preds).numpy()
    targets = torch.cat(all_targets).numpy()
    uncs = torch.cat(all_uncs).numpy()

    return _compute_metrics(preds, targets, uncs, loss_mode="factor")


# =============================================================================
# Main orchestrator
# =============================================================================


def run_single_split(
    split_id: int,
    dataset: DatasetPrediction,
) -> list[dict]:
    """
    Runs all six model variants on a single train/val/test split.

    Args:
        split_id: Split index (used for random seed).
        dataset: Full DatasetPrediction.

    Returns:
        List of 6 dicts, one per variant, each containing split_id,
        variant name, and all computed metrics.
    """
    total = len(dataset)
    val_size = int(total * VAL_RATIO)
    test_size = int(total * TEST_RATIO)
    train_size = total - val_size - test_size

    # Deterministic split using split_id as seed offset
    generator = torch.Generator().manual_seed(SEED + split_id)
    indices = torch.randperm(total, generator=generator).tolist()
    train_idx = indices[:train_size]
    val_idx = indices[train_size : train_size + val_size]
    test_idx = indices[train_size + val_size :]

    results = []

    # --- 1. Baseline (direct) ---
    print(f"  Split {split_id}: Baseline (direct)...")
    metrics = run_baseline(dataset, train_idx, val_idx, test_idx)
    metrics["split"] = split_id
    metrics["variant"] = "baseline"
    results.append(metrics)

    # --- 2. PCA ---
    print(f"  Split {split_id}: PCA...")
    metrics = run_deterministic_latent("pca", dataset, train_idx, val_idx, test_idx)
    metrics["split"] = split_id
    metrics["variant"] = "pca"
    results.append(metrics)

    # --- 3. KPCA ---
    print(f"  Split {split_id}: KPCA...")
    metrics = run_deterministic_latent("kpca", dataset, train_idx, val_idx, test_idx)
    metrics["split"] = split_id
    metrics["variant"] = "kpca"
    results.append(metrics)

    # --- 4. ICA ---
    print(f"  Split {split_id}: ICA...")
    metrics = run_deterministic_latent("ica", dataset, train_idx, val_idx, test_idx)
    metrics["split"] = split_id
    metrics["variant"] = "ica"
    results.append(metrics)

    # --- 5. VAE (no context) ---
    print(f"  Split {split_id}: VAE (no context)...")
    metrics = run_vae_variant(
        dataset, train_idx, val_idx, test_idx, include_context=False
    )
    metrics["split"] = split_id
    metrics["variant"] = "vae_no_context"
    results.append(metrics)

    # --- 6. VAE + Context (Final) ---
    print(f"  Split {split_id}: VAE + Context (Final)...")
    metrics = run_vae_variant(
        dataset, train_idx, val_idx, test_idx, include_context=True
    )
    metrics["split"] = split_id
    metrics["variant"] = "vae_final"
    results.append(metrics)

    return results


def format_table(summary: pd.DataFrame) -> str:
    """
    Formats the summary DataFrame as an ASCII table matching Table 3 layout.

    Columns: Variant | MAE ↓ | MSE ↓ | Pearson ρ ↑ | ℒ_pred ↓

    Values are reported as "mean (std)" across splits.
    """
    # Group by variant category for the table header
    header = (
        f"{'Metric':<18}"
        f"{'Baseline':>16}"
        f"{'PCA':>16}"
        f"{'KPCA':>16}"
        f"{'ICA':>16}"
        f"{'VAE':>16}"
        f"{'Final':>16}"
    )
    separator = "-" * len(header)

    rows = []
    for metric, symbol in [
        ("mae", "MAE ↓"),
        ("mse", "MSE ↓"),
        ("corr", "Pearson ρ ↑"),
        ("loss_pred", "ℒ_pred ↓"),
    ]:
        row_str = f"{symbol:<18}"
        for variant in [
            "baseline",
            "pca",
            "kpca",
            "ica",
            "vae_no_context",
            "vae_final",
        ]:
            sub = summary[summary["variant"] == variant]
            mean_val = sub[metric].mean()
            std_val = sub[metric].std()
            row_str += f"{mean_val:>8.2f} ({std_val:.2f})"
        rows.append(row_str)

    return "\n".join([separator, header, separator] + rows + [separator])


def main():
    """
    Main entry point for the cross-validation model comparison.

    Workflow:
    1. Load or create the unified prediction dataset
    2. For each of N_SPLITS random partitions:
       a. Train and evaluate all 6 model variants
       b. Collect per-split metrics
    3. Aggregate results into mean (std) summary
    4. Print formatted Table 3 and save CSV
    """
    print("=" * 70)
    print("CROSS-VALIDATION MODEL COMPARISON (Table 3)")
    print("=" * 70)
    print(f"Splits: {N_SPLITS}")
    print(f"Train/Val/Test: {TRAIN_RATIO:.0%}/{VAL_RATIO:.0%}/{TEST_RATIO:.0%}")
    print(f"Device: {DEVICE}")
    print(f"Latent dim: {LATENT_DIM}")
    print()

    # Set seeds
    random.seed(SEED)
    np.random.seed(SEED)
    torch.manual_seed(SEED)

    # Load dataset
    dataset = get_or_create_dataset()
    print(f"Dataset: {len(dataset)} samples")
    print(f"  Input features: {dataset.input_df.shape[1]}")
    print(f"  Context features: {dataset.context_df.shape[1]}")
    print(f"  Emission sectors: {dataset.emi_df.shape[1]}")
    print()

    # Run all splits
    all_results = []
    for split_id in range(N_SPLITS):
        print(f"\n{'=' * 50}")
        print(f"SPLIT {split_id + 1}/{N_SPLITS}")
        print(f"{'=' * 50}")
        split_results = run_single_split(split_id, dataset)
        all_results.extend(split_results)

        # Periodic progress summary
        if (split_id + 1) % 10 == 0:
            df_so_far = pd.DataFrame(all_results)
            print(f"\n--- Progress after {split_id + 1} splits ---")
            for variant in df_so_far["variant"].unique():
                sub = df_so_far[df_so_far["variant"] == variant]
                print(
                    f"  {variant:>20}: MAE={sub['mae'].mean():.3f} "
                    f"MSE={sub['mse'].mean():.3f} "
                    f"ρ={sub['corr'].mean():.3f} "
                    f"ℒ={sub['loss_pred'].mean():.3f}"
                )

    # Aggregate and save
    results_df = pd.DataFrame(all_results)

    OUTPUT_DIR.mkdir(parents=True, exist_ok=True)
    output_path = OUTPUT_DIR / "table3_cv_comparison.csv"
    results_df.to_csv(output_path, index=False)
    print(f"\nFull results saved to {output_path}")

    # Print formatted table
    print("\n" + "=" * 70)
    print("TABLE 3: Performance comparison across model variants")
    print(
        "All metrics reported as mean (std) across "
        f"{N_SPLITS} random train/val/test "
        f"({TRAIN_RATIO:.0%}/{VAL_RATIO:.0%}/{TEST_RATIO:.0%}) splits."
    )
    print("=" * 70)
    print(format_table(results_df))

    # Also print per-variant summary
    print("\n\nDetailed summary:")
    for variant in ["baseline", "pca", "kpca", "ica", "vae_no_context", "vae_final"]:
        sub = results_df[results_df["variant"] == variant]
        print(f"\n  {variant}:")
        for metric in ["mae", "mse", "corr", "loss_pred"]:
            print(
                f"    {metric:>10}: {sub[metric].mean():.4f} ± {sub[metric].std():.4f}"
            )


if __name__ == "__main__":
    # Must be set before importing torch-dependent modules
    mp.set_start_method("spawn", force=True)
    main()
